# deepL_beyond_infwidth

This code trains different multilayer perceptrons on regression tasks with quadratic loss function in a teacher-student setting.

The script to be run is ```deep_regression.py```.  

### Arguments

The **mandatory** arguments that must passed to ```deep_regression.py```:

1. **teacher type**. This is the function that will generate the input-output distribution. It can be chosen among: 
  - ```linear``` (linear function) 
  - ```random``` (random data and labels)
  - ```mnist``` (MNIST dataset of handwritten digits)
 
  

The optional arguments that can be passed are: 
  - ```-N``` (size of input data)
  - ```-L``` (depth of the network)
  - ```-N1``` (size of the hidden layer(s)) 
  - ```-N2``` (size of the second hidden layer) 
  - ```-N1T``` (size of teacher hidden layer) 
  - ```-lr``` (learning rate)
  - ```-wd``` (weight decay)
  - ```-checkpoint``` (number of epochs between two checkpoints. at every checkpoint the state of the network is saved.)
  - ```-resume``` (set true if you wish to resume training from a previous checkpoint)
  - ```-P_norm``` (number of points to compute trivial predictor)
  - ```-P_test``` (number of examples in the training set) 
  - ```-verbose``` (set true if you wish to print test and train loss at every epoch) 
  - ```-fb``` (set true if you want full batch learning)
  - ```-bs``` (batch size)
  - ```-epochs``` (number training epochs)
  - ```-R``` (a run index. the default value is 0, you can change it if you want more than one run with the same parameters)
  - ```-P_start```
  - ```-step```
  - ```-nPoints```
  
This script will produce a number ```nPoints``` of runs, each run consists of training the network on P examples. 
P increases with ```nPoints```. In particular the first run will have ```P_start``` elements in the training set, and the following runs will have ```P_start * step```
elements. 
All these runs share the same teacher (the data will be drawn from the same distribution). 

### Code output

```deep_regression.py``` will create folders and output files in the home folder. Change ```mother_dir``` at line ```152``` if you want to modifiy the output folder. The script will create folders and subfolders named with the arguments used.
The script outputs 4 files per run (saved in the aforementioned folders). The first two are ```data_P_(value_of_P)_(run_attributes)``` and ```labels_P_(value_of_P)_(run_attributes)```,containing respectively train data and labels. The second one contains train loss, test loss and generalisation gap for each epoch. This will be named ```run_P_(value_of_P)_(run_attributes)```. At each checkpoint another file is produced, containing the network state:```solution_P_(value_of_P)_(run_attributes)```. 
 (If mnist is  chosen as the teacher, the ```labels_``` file will not be created)
 
### Example command prompt line
```
python deep_regression.py quadratic 1hl -N 100 -N1 300
```
This will train a 1hl architecture on a dataset generated by a quadratic teacher. The input dimension is 100 and the size of the hidden layer is 300.
  
  
